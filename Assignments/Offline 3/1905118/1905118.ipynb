{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load test dataset separately\n",
    "test_dataset = datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANT SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed all\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "    def parameters(self):\n",
    "            return []\n",
    "    \n",
    "    def info(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, in_dimension, out_dimension):\n",
    "        super().__init__()\n",
    "        # Initialize weights with HE initialization [Better for ReLU activation]\n",
    "        self.weights = np.random.randn(in_dimension, out_dimension) * np.sqrt(2.0 / in_dimension)\n",
    "        # random bias initialization\n",
    "        self.bias = np.random.randn(out_dimension)\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = np.dot(grad_output, self.weights.T)\n",
    "        self.grad_weights = np.dot(self.input.T, grad_output)\n",
    "        self.grad_bias = np.sum(grad_output, axis=0)        \n",
    "        return self.grad_input\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [\n",
    "            {'params': self.weights, 'grads': self.grad_weights},\n",
    "            {'params': self.bias, 'grads': self.grad_bias}\n",
    "        ]\n",
    "    \n",
    "    def info(self):\n",
    "        return [\n",
    "            {\n",
    "                'name': 'Dense',\n",
    "                'in_dimension': self.weights.shape[0],\n",
    "                'out_dimension': self.weights.shape[1],\n",
    "                'weights': self.weights,\n",
    "                'bias': self.bias\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(input, 0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output * (self.input > 0)\n",
    "        return self.grad_input\n",
    "\n",
    "    def info(self):\n",
    "        return [\n",
    "            {\n",
    "                'name': 'ReLU'\n",
    "            }\n",
    "        ]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, drop_probability):\n",
    "        super().__init__()\n",
    "        # p must be between 0 and 1\n",
    "        assert 0 <= drop_probability <= 1\n",
    "        self.keep_probabalility = 1.0 - drop_probability\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            self.mask = np.random.binomial(1, self.keep_probabalility, input.shape) / self.keep_probabalility\n",
    "            return input * self.mask\n",
    "        else:\n",
    "            return input\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output * self.mask\n",
    "        return self.grad_input\n",
    "    \n",
    "    def info(self):\n",
    "        return [\n",
    "            {\n",
    "                'name': 'Dropout',\n",
    "                'drop_probability': 1.0 - self.keep_probabalility             \n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, dimension, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.gamma = np.ones(dimension)\n",
    "        self.beta = np.zeros(dimension)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = 1e-8\n",
    "        self.count = 0\n",
    "        self.running_miu = 0\n",
    "        self.running_var = 0\n",
    "        self.grad_gamma = None\n",
    "        self.grad_beta = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            self.miuB = np.mean(input, axis=0)\n",
    "            self.x_minus_miuB = input - self.miuB\n",
    "            self.varB_actual = np.var(input, axis=0)\n",
    "            self.varB = self.varB_actual + self.epsilon\n",
    "            self.stdB = np.sqrt(self.varB)\n",
    "            self.norm = (input - self.miuB) / self.stdB\n",
    "\n",
    "            self.running_miu = self.momentum * self.running_miu + (1 - self.momentum) * self.miuB\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.varB_actual\n",
    "            self.count = max(input.shape[0], self.count)\n",
    "            \n",
    "            return self.gamma * self.norm + self.beta\n",
    "        else:\n",
    "            new_var = self.running_var * (self.count / (self.count - 1))\n",
    "            factor = self.gamma / np.sqrt(new_var + self.epsilon)\n",
    "            return factor * input + (self.beta - self.running_miu * factor)\n",
    "            \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_beta = np.sum(grad_output, axis=0)\n",
    "        self.grad_gamma = np.sum(grad_output * self.norm, axis=0)\n",
    "        self.grad_norm = grad_output * self.gamma\n",
    "        self.grad_varB = np.sum(self.grad_norm * self.x_minus_miuB * -0.5 * self.varB ** -1.5, axis=0)\n",
    "        self.grad_miuB = np.sum(self.grad_norm * -1 / self.stdB, axis=0) + self.grad_varB * np.mean(-2 * self.x_minus_miuB, axis=0)\n",
    "        self.grad_input = self.grad_norm / self.stdB + self.grad_varB * 2 * self.x_minus_miuB / grad_output.shape[0] + self.grad_miuB / grad_output.shape[0]\n",
    "        return self.grad_input\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [\n",
    "            {'params': self.gamma, 'grads': self.grad_gamma},\n",
    "            {'params': self.beta, 'grads': self.grad_beta},\n",
    "        ]\n",
    "    \n",
    "    def info(self):\n",
    "        return [\n",
    "            {\n",
    "                'name': 'BatchNorm',\n",
    "                'gamma': self.gamma,\n",
    "                'beta': self.beta,\n",
    "                'running_miu': self.running_miu,\n",
    "                'running_var': self.running_var,\n",
    "                'count': self.count,\n",
    "                'momentum': self.momentum\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output /= np.sum(self.output, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = np.zeros_like(grad_output)\n",
    "        for i in range(grad_output.shape[0]):\n",
    "            tmp = np.diag(self.output[i]) - np.outer(self.output[i], self.output[i])\n",
    "            self.grad_input[i] = np.dot(tmp, grad_output[i])\n",
    "        return self.grad_input\n",
    "    \n",
    "    def info(self):\n",
    "        return [\n",
    "            {\n",
    "                'name': 'SoftMax'\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate(self, y_true, y_pred):        \n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, layers, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layers = layers\n",
    "        self.parameters = self.params()\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.m = [np.zeros_like(param['params']) for param in self.parameters]\n",
    "        self.v = [np.zeros_like(param['params']) for param in self.parameters]\n",
    "        self.t = 0    \n",
    "\n",
    "    def params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "        \n",
    "    def step(self):\n",
    "        self.parameters = self.params()\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.parameters):\n",
    "            grads = param['grads']\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            param['params'] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "                \n",
    "    def save(self, filename):\n",
    "        infos = []\n",
    "        for layer in self.layers:\n",
    "            infos.extend(layer.info())\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(infos, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            infos = pickle.load(f)\n",
    "\n",
    "        self.layers = []\n",
    "        for info in infos:\n",
    "            if info['name'] == 'Dense':\n",
    "                layer = Dense(info['in_dimension'], info['out_dimension'])\n",
    "                layer.weights = info['weights']\n",
    "                layer.bias = info['bias']\n",
    "                self.layers.append(layer)\n",
    "            elif info['name'] == 'ReLU':\n",
    "                self.layers.append(ReLU())\n",
    "            elif info['name'] == 'Dropout':\n",
    "                self.layers.append(Dropout(info['drop_probability']))\n",
    "            elif info['name'] == 'BatchNorm':\n",
    "                layer = BatchNorm(info['gamma'].shape[0])\n",
    "                layer.gamma = info['gamma']\n",
    "                layer.beta = info['beta']\n",
    "                layer.running_miu = info['running_miu']\n",
    "                layer.running_var = info['running_var']\n",
    "                layer.count = info['count']\n",
    "                layer.momentum = info['momentum']\n",
    "                self.layers.append(layer)\n",
    "            elif info['name'] == 'SoftMax':\n",
    "                self.layers.append(SoftMax())\n",
    "\n",
    "    def summary(self):\n",
    "        for layer in self.layers:\n",
    "            info = layer.info()[0]\n",
    "            if info['name'] == 'Dense':\n",
    "                print(f\"Dense({info['in_dimension']}, {info['out_dimension']})\")\n",
    "            elif info['name'] == 'ReLU':\n",
    "                print('ReLU()')\n",
    "            elif info['name'] == 'Dropout':\n",
    "                print(f\"Dropout({info['drop_probability']})\")\n",
    "            elif info['name'] == 'BatchNorm':\n",
    "                print(f\"BatchNorm({info['gamma'].shape[0]})\")\n",
    "            elif info['name'] == 'SoftMax':\n",
    "                print('SoftMax()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Specific Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = 28 * 28\n",
    "output_dimension = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters, Model and File Paths Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01 \n",
    "\n",
    "# depends on how many models are defined\n",
    "# in my case there 3 models, so possible values are 0, 1, 2\n",
    "model_id = 0\n",
    "\n",
    "model_path = f'model_{model_id}_{lr}.pickle'\n",
    "metrics_base_path = f'metrics_{model_id}_{lr}'\n",
    "confusion_matrix_path = f'confusion_matrix_{model_id}_{lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model with no BatchNorm, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Model()\n",
    "\n",
    "model0.add(Dense(input_dimension, 256))\n",
    "model0.add(ReLU())\n",
    "\n",
    "model0.add(Dense(256, 128))\n",
    "model0.add(ReLU())\n",
    "\n",
    "model0.add(Dense(128, output_dimension))\n",
    "\n",
    "model0.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit deep model without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model()\n",
    "\n",
    "model1.add(Dense(input_dimension, 512))\n",
    "model1.add(BatchNorm(512))\n",
    "model1.add(ReLU())\n",
    "\n",
    "model1.add(Dense(512, 256))\n",
    "model1.add(BatchNorm(256))\n",
    "model1.add(ReLU())\n",
    "\n",
    "model1.add(Dense(256, 128))\n",
    "model1.add(BatchNorm(128))\n",
    "model1.add(ReLU())\n",
    "\n",
    "model1.add(Dense(128, output_dimension))\n",
    "\n",
    "model1.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep model with everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model()\n",
    "\n",
    "model2.add(Dense(input_dimension, 512))\n",
    "model2.add(BatchNorm(512))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(512, 256))\n",
    "model2.add(BatchNorm(256))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(256, 256))\n",
    "model2.add(BatchNorm(256))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(256, 128))\n",
    "model2.add(BatchNorm(128))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(Dense(128, 128))\n",
    "model2.add(BatchNorm(128))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "model2.add(Dense(128, 128))\n",
    "model2.add(BatchNorm(128))\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "model2.add(Dense(128, output_dimension))\n",
    "\n",
    "model2.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.extend([model0, model1, model2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[model_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "adam = Adam(model.layers, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = train_dataset.data.numpy().reshape(-1, input_dimension) / 255.0\n",
    "y_data = np.eye(10)[train_dataset.targets.numpy()]\n",
    "\n",
    "X_test = test_dataset.data.numpy().reshape(-1, input_dimension) / 255.0\n",
    "y_test = np.eye(10)[test_dataset.targets.numpy()]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_data, y_data, train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation data shape: {X_valid.shape}, {y_valid.shape}')\n",
    "print(f'Test data shape: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "best_macro_f1 = -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop and Metric Calculation with Validation at every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to train the model\n",
    "\n",
    "# for epoch in range(num_epochs):    \n",
    "#     # Shuffle training data\n",
    "#     indices = np.arange(X_train.shape[0])\n",
    "#     np.random.shuffle(indices)\n",
    "#     X_train = X_train[indices]\n",
    "#     y_train = y_train[indices]\n",
    "\n",
    "#     model.train()\n",
    "#     loss_value = 0\n",
    "\n",
    "#     # Training loop\n",
    "#     for i in range(0, X_train.shape[0], batch_size):\n",
    "#         X_batch = X_train[i:i + batch_size]\n",
    "#         y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "#         # Forward pass\n",
    "#         y_pred = model.forward(X_batch)\n",
    "\n",
    "#         # Compute loss\n",
    "#         current_loss = loss.calculate(y_batch, y_pred)\n",
    "#         loss_value += current_loss\n",
    "\n",
    "#         # Compute loss gradient\n",
    "#         loss_grad = -y_batch / (y_pred + 1e-8)\n",
    "\n",
    "#         # Backward pass\n",
    "#         model.backward(loss_grad)\n",
    "\n",
    "#         # Update parameters\n",
    "#         adam.step()\n",
    "        \n",
    "#     # Average training loss\n",
    "#     loss_value /= X_train.shape[0]\n",
    "\n",
    "#     # Training accuracy\n",
    "#     model.eval()\n",
    "#     y_train_pred = model.forward(X_train)\n",
    "#     train_accuracy = np.mean(\n",
    "#         np.argmax(y_train_pred, axis=1) == np.argmax(y_train, axis=1)\n",
    "#     )\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     y_valid_pred = model.forward(X_valid)\n",
    "#     valid_loss = loss.calculate(y_valid, y_valid_pred) / X_valid.shape[0]\n",
    "#     valid_accuracy = np.mean(\n",
    "#         np.argmax(y_valid_pred, axis=1) == np.argmax(y_valid, axis=1)\n",
    "#     )\n",
    "\n",
    "#     # decrease learning rate\n",
    "#     adam.lr *= 0.98\n",
    "    \n",
    "#     y_valid_true = np.argmax(y_valid, axis=1)\n",
    "#     y_valid_pred_labels = np.argmax(y_valid_pred, axis=1)\n",
    "#     valid_macro_f1 = f1_score(y_valid_true, y_valid_pred_labels, average='macro')                   \n",
    "\n",
    "#     if epoch % 10 == 9:\n",
    "#         print(\n",
    "#             f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "#             f'Train Loss: {loss_value:.4f}, '\n",
    "#             f'Train Accuracy: {train_accuracy:.4f}, '\n",
    "#             f'Validation Loss: {valid_loss:.4f}, '\n",
    "#             f'Validation Accuracy: {valid_accuracy:.4f}, '\n",
    "#             f'Validation Macro-F1: {valid_macro_f1:.4f}'\n",
    "#         )\n",
    "\n",
    "#     metrics.append([\n",
    "#         epoch + 1,\n",
    "#         f\"{loss_value:.4f}\",\n",
    "#         f\"{train_accuracy:.4f}\",\n",
    "#         f\"{valid_loss:.4f}\",\n",
    "#         f\"{valid_accuracy:.4f}\",\n",
    "#         f\"{valid_macro_f1:.4f}\"\n",
    "#     ])\n",
    "\n",
    "#     if valid_macro_f1 > best_macro_f1:        \n",
    "#         print(f'Epoch {epoch + 1} - Saving model')\n",
    "#         best_macro_f1 = valid_macro_f1\n",
    "#         model.save(model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving metrics as pretty table in a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to save the metrics\n",
    "\n",
    "# headers = [\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Validation Loss\", \"Validation Accuracy\", \"Validation Macro-F1\"]\n",
    "\n",
    "# table = tabulate(metrics, headers=headers, tablefmt=\"pretty\")\n",
    "\n",
    "# with open(f'{metrics_base_path}.txt', \"w\") as f:\n",
    "#     f.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save metrics as Pandas Dataframe with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to save the metrics as CSV\n",
    "\n",
    "# df = pd.DataFrame(metrics, columns=headers)\n",
    "\n",
    "# df.to_csv(f'{metrics_base_path}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary plotting of Train and Validation Loss and Accuracy vs Epochs\n",
    "# Plotting the Validation Macro F1 Score vs Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to plot the metrics\n",
    "\n",
    "# # Ensure the columns are numeric\n",
    "# df['Epoch'] = pd.to_numeric(df['Epoch'])\n",
    "# df['Train Accuracy'] = pd.to_numeric(df['Train Accuracy'])\n",
    "# df['Validation Accuracy'] = pd.to_numeric(df['Validation Accuracy'])\n",
    "# df['Train Loss'] = pd.to_numeric(df['Train Loss'])\n",
    "# df['Validation Loss'] = pd.to_numeric(df['Validation Loss'])\n",
    "# df['Validation Macro-F1'] = pd.to_numeric(df['Validation Macro-F1'])\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.plot(df['Epoch'], df['Train Accuracy'], color='green', label='Train Accuracy')\n",
    "# plt.plot(df['Epoch'], df['Validation Accuracy'], color='red', label='Validation Accuracy')\n",
    "\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.ylim(min(df['Train Accuracy'].min(), df['Validation Accuracy'].min()) * 0.8, 1)\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.plot(df['Epoch'], df['Train Loss'], color='green', label='Train Loss')\n",
    "# plt.plot(df['Epoch'], df['Validation Loss'], color='red', label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.ylim(min(df['Train Loss'].min(), df['Validation Loss'].min()) * 0.8, max(df['Train Loss'].max(), df['Validation Loss'].max()) * 1.2)\n",
    "\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.plot(df['Epoch'], df['Validation Macro-F1'], color='blue', label='Validation Macro-F1')\n",
    "\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Macro F1 Score')\n",
    "# plt.title('Validation Macro-F1 Score')\n",
    "# plt.ylim(df['Validation Macro-F1'].min() * 0.8, 1)\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # save the plot\n",
    "# plt.savefig(f'{metrics_base_path}.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = Model()\n",
    "# model_val.load(model_path)\n",
    "\n",
    "model_val.load('model_1905118.pickle')\n",
    "model.eval()\n",
    "\n",
    "y_pred = model.forward(X_valid)\n",
    "\n",
    "y_true = np.argmax(y_valid, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_labels)\n",
    "\n",
    "conf_matrix_str = tabulate(conf_matrix, tablefmt='grid')\n",
    "\n",
    "with open(f'{confusion_matrix_path}_valid.txt', \"w\") as f:\n",
    "    f.write(conf_matrix_str)\n",
    "\n",
    "print(conf_matrix_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model and Getting Summary of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Model()\n",
    "# model.load(model_path)\n",
    "\n",
    "# Loading best model\n",
    "model.load('model_1905118.pickle')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the test accuracy and macro F1 score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_pred = model.forward(X_test)\n",
    "\n",
    "test_accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "test_macro_f1_score = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "\n",
    "print(f'Test Accuracy : {test_accuracy:.4f}')\n",
    "print(f'Test Macro F1 Score: {test_macro_f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_labels)\n",
    "\n",
    "conf_matrix_str = tabulate(conf_matrix, tablefmt='grid')\n",
    "\n",
    "with open(f'{confusion_matrix_path}_test.txt', \"w\") as f:\n",
    "    f.write(conf_matrix_str)\n",
    "\n",
    "print(conf_matrix_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best on validation data\n",
    "# -------------Model 2 with learning rate 0.01-------------\n",
    "## Validation Accuracy: 0.9092\n",
    "## Validation Macro F1 Score: 0.9093\n",
    "# ---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
